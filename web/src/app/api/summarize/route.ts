// web/src/app/api/summarize/route.ts
// LLMå¿…é ˆãƒ»å …ç‰¢ç‰ˆï¼ˆå¸¸ã«ãƒ­ãƒ¼ã‚«ãƒ« LLM ã§è¦ç´„ã€‚å¤±æ•—=502ï¼‰
// - llama.cpp / Ollama ã® OpenAI äº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå„ªå…ˆ + Ollama /api/generate ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
// - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/ã‚¢ãƒœãƒ¼ãƒˆæ­£ã—ãå®Ÿè£…ã€ENV æ­£è¦è¡¨ç¾ã®å®‰å…¨ãƒ‘ãƒ¼ã‚¹ã€JSON æŠ½å‡ºã®å …ç‰¢åŒ–
// - oneLiner ãŒ URL/ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³/çµµæ–‡å­—ã ã‘ã«ãªã‚‹ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’è‡ªå‹•è£œæ­£
// - Next.js App Router ç”¨ã« runtime/dynamic ã‚’æŒ‡å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ« HTTP å¯ï¼‰

import { NextRequest, NextResponse } from "next/server";

// ---- Next.js å®Ÿè¡Œç’°å¢ƒæŒ‡å®š ----
export const runtime = "nodejs";
export const dynamic = "force-dynamic";

// ---- Types ----
export type RawMsg = { id: string; author: string; content: string; ts: string };

type Decision     = { what: string; who?: string; when?: string };
type ActionItem   = { owner?: string; task: string; due?: string };
type OpenQuestion = { asker?: string; q: string };
type Label        = { id: string; cat: "AG" | "EM" | "Q" | "TP" | "S" | "NG" | "CH" };

type PipelineOut = {
  oneLiner: string;
  practical: string;
  bullets: string[];
  decisions: Decision[];
  actionItems: ActionItem[];
  openQuestions: OpenQuestion[];
  labels: Label[];
  coverage: { coverageRate: number; total: number; used: number; missing?: any };
  meta?: { usedLlm: boolean; engine: string | null };
};

// ---- ENV ----
const ENV = {
  LLM_PROVIDER: (process.env.LLM_PROVIDER ?? "auto") as "auto" | "llama" | "ollama",

  // llama.cpp
  USE_LLAMA: (process.env.ANALYSIS_USE_LLAMA_CPP ?? "0") === "1",
  LLAMA_BASE: (process.env.LLAMA_BASE ?? "http://127.0.0.1:8080").replace(/\/+$/, ""),
  LLAMA_MODEL: process.env.LLAMA_MODEL || "local",

  // Ollama
  USE_OLLAMA: (process.env.ANALYSIS_USE_OLLAMA ?? "0") === "1",
  OLLAMA_BASE: (process.env.OLLAMA_BASE ?? "http://127.0.0.1:11434").replace(/\/+$/, ""),
  OLLAMA_MODEL: process.env.OLLAMA_MODEL || "qwen2.5:7b",

  // å‰å‡¦ç†
  SUM_MAX_MSG: Number(process.env.SUM_MAX_MSG ?? "500"),
  SUM_MSG_TRIM: Number(process.env.SUM_MSG_TRIM ?? "240"),
  SUM_SPAM_REGEX: process.env.SUM_SPAM_REGEX ?? "/(^@everyone\\b)|(^https?:\\/\\/\\S+$)/i",

  // LLM
  TEMP: Number(process.env.SUM_TEMPERATURE ?? "0.2"),
  MAX_TOKENS: Number(process.env.SUM_MAX_TOKENS ?? "1200"),

  // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
  REQ_TIMEOUT_MS: Number(process.env.SUM_TIMEOUT_MS ?? "25000"),
};

// ---- Utils ----
function pickEngine() {
  const order: Array<"llama" | "ollama"> =
    ENV.LLM_PROVIDER === "llama"  ? ["llama"] :
    ENV.LLM_PROVIDER === "ollama" ? ["ollama"] :
    ENV.USE_LLAMA && ENV.USE_OLLAMA ? ["llama", "ollama"] :
    ENV.USE_LLAMA ? ["llama"] :
    ENV.USE_OLLAMA ? ["ollama"] : [];

  for (const k of order) {
    if (k === "llama"  && ENV.USE_LLAMA)  return { kind: "llama"  as const, base: ENV.LLAMA_BASE,  model: ENV.LLAMA_MODEL };
    if (k === "ollama" && ENV.USE_OLLAMA) return { kind: "ollama" as const, base: ENV.OLLAMA_BASE, model: ENV.OLLAMA_MODEL };
  }
  return null;
}

function fetchWithTimeout(input: string, init: RequestInit = {}, ms = ENV.REQ_TIMEOUT_MS) {
  const ac = new AbortController();
  const t = setTimeout(() => ac.abort(), ms);
  const merged: RequestInit = { ...init, signal: ac.signal };
  return fetch(input, merged).finally(() => clearTimeout(t));
}

function safeJsonParse<T = any>(s: string): T | null {
  try { return JSON.parse(s); }
  catch {
    const m = s.match(/\{[\s\S]*\}/);
    if (!m) return null;
    try { return JSON.parse(m[0]); } catch { return null; }
  }
}

function normalizeText(x: string, max = ENV.SUM_MSG_TRIM) {
  const t = (x || "")
    .replace(/\r|\t/g, " ")
    .replace(/\s+/g, " ")
    .trim();
  return t.length > max ? t.slice(0, max - 1) + "â€¦" : t;
}

function cleanArtifacts(s: string) {
  return (s || "")
    .replace(/<\|[^>]*\|>/g, "")             // <|eot_id|> ç­‰
    .replace(/^\s*(assistant|system)\s*:\s*/i, "")
    .replace(/\s+/g, " ")
    .trim();
}

function isCrudTokenOnly(s: string) {
  // URLã€ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã€æ·»ä»˜ã€çµµæ–‡å­—ç­‰ã—ã‹å«ã¾ãªã„ãªã‚‰ true
  const t = (s || "").trim();
  if (!t) return true;
  const noText = t
    .replace(/https?:\/\/\S+/g, "")
    .replace(/<@[!&]?\d+>/g, "")
    .replace(/@[a-zA-Z0-9_\-]+/g, "")
    .replace(/[:*#\u{1F300}-\u{1FAFF}]/gu, "")
    .replace(/[ğŸ“ğŸ”—]/g, "")
    .replace(/[ãƒ»\s]/g, "");
  return noText.length === 0;
}

function toRegex(input: string): RegExp {
  // "/pattern/flags" ã¾ãŸã¯ "pattern" ã®ä¸¡æ–¹ã‚’è¨±å®¹
  const m = input.match(/^\/(.+)\/([gimsuy]*)$/);
  if (m) {
    try { return new RegExp(m[1], m[2] as any); } catch { /* fallthrough */ }
  }
  try { return new RegExp(input, "i"); } catch { return /$a/; } // ä¸ä¸€è‡´ãƒ€ãƒŸãƒ¼
}

function filterAndDedupe(raw: RawMsg[]) {
  const re = toRegex(ENV.SUM_SPAM_REGEX);
  const seen = new Map<string, number>();
  const out: RawMsg[] = [];

  for (const m of raw) {
    const text = (m.content || "").trim();
    if (!text) continue;
    if (re.test(text)) continue;

    const key = text
      .toLowerCase()
      .replace(/[ï¼!ã€‚ï¼.ã€,ã€œ~ï½—w\s]+/g, "")
      .replace(/https?:\/\/\S+/g, "");

    const n = (seen.get(key) ?? 0) + 1;
    if (n <= 3) out.push(m);     // åŒä¸€æ–‡3å›ã¾ã§è¨±å®¹
    seen.set(key, n);
  }
  return out;
}

// ---- Prompt ----
const CATEGORY_GUIDE = `
# ã‚«ãƒ†ã‚´ãƒªå®šç¾©ï¼ˆæœ€ã‚‚å¼·ã„1ã¤ã€åŒç‡ãªã‚‰2ã¤ã¾ã§ï¼‰
- TP(è©±é¡Œæç¤º): æ–°ã—ã„è©±é¡Œã®æŒã¡è¾¼ã¿ã€‚è­°è«–ã®ä½™åœ°ãŒã‚ã‚‹ã€‚
- Q(è³ªå•): å®Ÿè³ªçš„ãªè³ªå•ï¼ˆ?ã®æœ‰ç„¡ã¯ä¸å•ï¼‰ã€‚
- S(æƒ…å ±å…±æœ‰): äº‹å®Ÿãƒ»ãƒªãƒ³ã‚¯ãƒ»å¼•ç”¨ã®å…±æœ‰ï¼ˆåå¿œãŒãªãã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
- EM(æ„Ÿæƒ…): æ„Ÿæƒ…ãƒ»æ„Ÿæƒ³ãƒ»å…±æ„Ÿãƒ»é©šããƒ»è³è³›ãƒ»è¬ç½ªãªã©ã€‚
- AG(è³›åŒ): æ±ºå®šã‚„æ„è¦‹ã¸ã®è³›æˆ/åŒæ„/å…±æ„Ÿã€‚
- CH(é›‘è«‡/ãã®ä»–): ä¸Šè¨˜ä»¥å¤–ã®è»½è«‡ã€‚
- NG(ç„¡åŠ¹): æ˜ç¢ºãªã‚¹ãƒ‘ãƒ /ç„¡é–¢ä¿‚/æ©Ÿæ¢°ãƒã‚¤ã‚ºã€‚
1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¤ãåŸå‰‡1ï¼ˆåŒç‡ã§æœ€å¤§2ï¼‰ã¾ã§ã€‚ç°¡æ½”ã«ã€‚`;

const OUTPUT_SCHEMA = `
JSONã®ã¿ã§å‡ºåŠ›:
{
  "oneLiner": "ã€ã“ã®ä¼šè©±ã¯â—¯â—¯ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹ã€ã®å½¢å¼ï¼ˆå¥ç‚¹ãªã—ãƒ»å…¨è§’60å­—ä»¥å†…ï¼‰",
  "topics": ["ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯(æœ€å¤§5)"],
  "bullets": ["3ã€œ6è¡Œã®è¦ç‚¹ï¼ˆå„ å…¨è§’100å­—ä»¥å†…ï¼‰"],
  "decisions": [{"what":"ä½•ã‚’","who":"èª°ãŒ","when":"ã„ã¤"}],
  "actionItems": [{"owner":"æ‹…å½“","task":"ã‚¿ã‚¹ã‚¯","due":"æœŸé™(ç©ºå¯)"}],
  "openQuestions": [{"asker":"èª°","q":"è³ªå•æ–‡"}],
  "labels": [{"id":"msgId","cat":"TP|Q|S|EM|AG|CH|NG"}],
  "notes": "è£œè¶³ï¼ˆä»»æ„ã€çŸ­ãï¼‰"
}`;

function buildPrompt(channelId: string, logs: RawMsg[]) {
  const head = `ã‚ãªãŸã¯Discordã®ä¼šè©±è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
- äº‹å®Ÿã«å¿ å®Ÿã«ã€å…·ä½“èªã§ç°¡æ½”ã«ã€‚
- URL/@everyone/çµµæ–‡å­—ã ã‘ã®ç™ºè¨€ã¯ç„¡è¦–ã€‚
- é‡è¤‡ã¯ä»£è¡¨ä¾‹ã®ã¿ã€‚
- ${CATEGORY_GUIDE}

${OUTPUT_SCHEMA}

# å…¥åŠ›ï¼ˆå¤ã„â†’æ–°ã—ã„ï¼‰
id | author | ts | text`;

  const body = logs
    .map(m => `${m.id} | ${m.author} | ${m.ts} | ${normalizeText(m.content)}`)
    .join("\n");

  return `${head}\n${body}`;
}

// ---- LLM calls ----
async function callChatCompat(base: string, model: string, messages: Array<{role:"system"|"user";content:string}>) {
  try {
    const r = await fetchWithTimeout(`${base}/v1/chat/completions`, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify({ model, messages, temperature: ENV.TEMP, max_tokens: ENV.MAX_TOKENS }),
    });
    if (!r.ok) return null;
    const j: any = await r.json().catch(() => ({}));
    return (j?.choices?.[0]?.message?.content ?? null) as string | null;
  } catch { return null; }
}

async function callOllamaGenerate(base: string, model: string, prompt: string) {
  try {
    const r = await fetchWithTimeout(`${base}/api/generate`, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify({ model, prompt, options: { temperature: ENV.TEMP, num_predict: ENV.MAX_TOKENS } }),
    });
    if (!r.ok) return null;
    const text = await r.text();
    const lines = text.trim().split(/\r?\n/);
    for (let i = lines.length - 1; i >= 0; i--) {
      try {
        const j = JSON.parse(lines[i]);
        if (j?.response) return j.response as string;
      } catch {}
    }
    return null;
  } catch { return null; }
}

async function runLLM(prompt: string): Promise<{ parsed: any; engine: string } | null> {
  const plan = pickEngine();
  if (!plan) return null;

  // 1) chat/completions
  const content = await callChatCompat(plan.base, plan.model, [
    { role: "system", content: "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªè­°äº‹è¦ç´„è€…ã§ã™ã€‚å¿…ãšJSONã®ã¿ã‚’è¿”ã—ã¾ã™ã€‚" },
    { role: "user", content: prompt },
  ]);
  if (content) {
    const parsed = safeJsonParse(content);
    if (parsed) return { parsed, engine: plan.kind };
  }

  // 2) ollama generate fallback
  if (plan.kind === "ollama") {
    const text = await callOllamaGenerate(plan.base, plan.model, `${prompt}\n\nJSONã®ã¿ã§å›ç­”ã€‚`);
    const parsed = text ? safeJsonParse(text) : null;
    if (parsed) return { parsed, engine: plan.kind };
  }

  // 3) alternate engine (auto)
  if (ENV.LLM_PROVIDER === "auto") {
    const alt = plan.kind === "llama"
      ? { kind: "ollama" as const, base: ENV.OLLAMA_BASE, model: ENV.OLLAMA_MODEL, ok: ENV.USE_OLLAMA }
      : { kind: "llama"  as const, base: ENV.LLAMA_BASE,  model: ENV.LLAMA_MODEL,  ok: ENV.USE_LLAMA  };

    if (alt.ok) {
      const c2 = await callChatCompat(alt.base, alt.model, [
        { role: "system", content: "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªè­°äº‹è¦ç´„è€…ã§ã™ã€‚å¿…ãšJSONã®ã¿ã‚’è¿”ã—ã¾ã™ã€‚" },
        { role: "user", content: prompt },
      ]);
      if (c2) {
        const parsed = safeJsonParse(c2);
        if (parsed) return { parsed, engine: alt.kind };
      }
      if (alt.kind === "ollama") {
        const t2 = await callOllamaGenerate(alt.base, alt.model, `${prompt}\n\nJSONã®ã¿ã§å›ç­”ã€‚`);
        const p2 = t2 ? safeJsonParse(t2) : null;
        if (p2) return { parsed: p2, engine: alt.kind };
      }
    }
  }

  return null;
}

// ---- Embed payload ----
function toEmbedPayload(channelId: string, msgsAll: RawMsg[], msgsUsed: RawMsg[], modelOut: any, engine: string | null): PipelineOut {
  const total = msgsAll.length;
  const used  = msgsUsed.length;
  const coverageRate = total > 0 ? used / total : 0;

  let one = cleanArtifacts(modelOut?.oneLiner || modelOut?.title || "");
  if (!one || isCrudTokenOnly(one)) {
    const topics: string[] = Array.isArray(modelOut?.topics) ? modelOut.topics : [];
    const t = topics?.[0]?.trim();
    one = t && !isCrudTokenOnly(t) ? `ã“ã®ä¼šè©±ã¯${t}ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹` : "ã“ã®ä¼šè©±ã¯é›‘è«‡ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹";
  }
  one = one.replace(/ã€‚+$/,"").slice(0, 60);

  const bullets: string[]      = Array.isArray(modelOut?.bullets) ? modelOut.bullets : [];
  const topics:  string[]      = Array.isArray(modelOut?.topics)  ? modelOut.topics  : [];
  const decisions: any[]       = Array.isArray(modelOut?.decisions) ? modelOut.decisions : [];
  const actionItems: any[]     = Array.isArray(modelOut?.actionItems) ? modelOut.actionItems : [];
  const openQuestions: any[]   = Array.isArray(modelOut?.openQuestions) ? modelOut.openQuestions : [];
  const labels: any[]          = Array.isArray(modelOut?.labels) ? modelOut.labels : [];

  const practical =
    [
      ...topics.slice(0, 3).map(t => `ãƒ»${normalizeText(t, 80)}`),
      ...bullets.slice(0, 6).map(b => `ãƒ»${normalizeText(b, 100)}`),
    ].join("\n") || "â€”";

  return {
    oneLiner: one || "ï¼ˆä¸»é¡Œã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰",
    practical,
    bullets: bullets.slice(0, 10),
    decisions: decisions.slice(0, 10),
    actionItems: actionItems.slice(0, 10),
    openQuestions: openQuestions.slice(0, 10),
    labels: labels.slice(0, 200),
    coverage: { total, used, coverageRate },
    meta: { usedLlm: true, engine },
  };
}

// ---- Handler ----
export async function POST(req: NextRequest) {
  try {
    const body = await req.json().catch(() => ({}));
    const channelId = String(body?.channelId ?? "");
    const messages: RawMsg[] = Array.isArray(body?.messages) ? body.messages : [];

    if (!channelId || !messages.length) {
      return NextResponse.json({ error: "invalid payload: channelId and messages are required" }, { status: 400 });
    }

    const plan = pickEngine();
    if (!plan) {
      return NextResponse.json({
        error: "No local LLM engine configured.",
        hint: {
          set: {
            llama:  { ANALYSIS_USE_LLAMA_CPP: "1", LLAMA_BASE: ENV.LLAMA_BASE, LLAMA_MODEL: ENV.LLAMA_MODEL },
            ollama: { ANALYSIS_USE_OLLAMA: "1", OLLAMA_BASE: ENV.OLLAMA_BASE, OLLAMA_MODEL: ENV.OLLAMA_MODEL },
            LLM_PROVIDER: "auto|llama|ollama"
          }
        }
      }, { status: 503 });
    }

    // å‰å‡¦ç†ï¼šãƒã‚¤ã‚ºæŠ‘åˆ¶ + ç›´è¿‘ N ä»¶ï¼ˆå¤â†’æ–°ï¼‰
    const filtered = filterAndDedupe(messages);
    const sliced   = filtered.slice(-ENV.SUM_MAX_MSG);
    if (sliced.length === 0) {
      return NextResponse.json({
        error: "no usable messages after filtering",
        coverage: { total: messages.length, used: 0, coverageRate: 0, missing: { reason: "filtered_out" } }
      }, { status: 422 });
    }

    // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ LLM
    const prompt = buildPrompt(channelId, sliced);
    const result = await runLLM(prompt);
    if (!result?.parsed) {
      return NextResponse.json({
        error: "LLM summarize failed (timeout or invalid response).",
        hint: {
          engineOrder: ENV.LLM_PROVIDER,
          llama:  ENV.USE_LLAMA  ? `${ENV.LLAMA_BASE}/v1/chat/completions` : "disabled",
          ollama: ENV.USE_OLLAMA ? `${ENV.OLLAMA_BASE}/v1/chat/completions|/api/generate` : "disabled",
          timeoutMs: ENV.REQ_TIMEOUT_MS
        }
      }, { status: 502 });
    }

    const payload = toEmbedPayload(channelId, messages, sliced, result.parsed, result.engine);
    return NextResponse.json(payload, { status: 200 });

  } catch (e: any) {
    console.error("[/api/summarize] fatal:", e?.stack || e);
    return NextResponse.json({ error: String(e?.message || e) }, { status: 500 });
  }
}
